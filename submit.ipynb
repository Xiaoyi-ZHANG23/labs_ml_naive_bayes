{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, create a new Jupyther notebook, and implement a module that reads files and stores their content in 2 string arrays of file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os\n",
    "  \n",
    "# Folder Path\n",
    "path = \"C:/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/movies_reviews/movies_reviews\"\n",
    "negpath = path +\"/neg/\"\n",
    "pospath = path + \"/pos\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(negpath)\n",
    "  \n",
    "# Read text File\n",
    "  \n",
    "  \n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string\n",
    "    \n",
    "\n",
    "negstringarray = []\n",
    "negfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{negpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        negstringarray.append(string)\n",
    "        negfilearray.append(file)\n",
    "os.chdir(pospath)        \n",
    "posstringarray = []\n",
    "posfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{pospath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        posstringarray.append(string)\n",
    "        posfilearray.append(file)\n",
    "#print(array)\n",
    "newpath = path + \"/sample_new_reviews\"\n",
    "os.chdir(newpath)        \n",
    "newstringarray = []\n",
    "newfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{newpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        newstringarray.append(string)\n",
    "        newfilearray.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, you would need to convert the words in each document into a vector of word occurrences. \n",
    "You can use the code with stop words from the clustering demo or you can use the `sklearn` module `feature_extraction.text`, \n",
    "where you are interested in the `CountVectorizer` (for this one you would need to remove stop words) or in the `TfidfTransformer`. \n",
    "The latter assigns a score to each word based on its frequencies across all the documents, \n",
    "and thus the words that occur across all the documents (the stop words) get score zero, so there is no need to remove stop words. You can find a nice explanation and an example about tf/idf score [here](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3). Whatever vectorization technique you chose, you would need to explain it in your own words in a separate markdown cell in your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination: The method reads in the strings and build a disctionary after excluding the \"build-in\" stop words, which are the words that are meaningless. It then converts the string into the frequency array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2005, 39373)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(negstringarray + posstringarray)\n",
    "\n",
    "#print(X.toarray())\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a vector for each review, you can add the labels *pos* or *neg*, depending on the directory (as we did in cat/dog classification demo), and then divide the dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At position 3 should be a neg: 1.0\n",
      "At position 2002 should be a pos: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2005,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y_train_orig = np.ones((1000,)) # 1 - 1000 are neg reviews so our label is 1\n",
    "Y_train_orig = np.concatenate((Y_train_orig, np.zeros((1005,)))) # 1000 - 20005 are pos reviews so our label is 0\n",
    "Y = Y_train_orig.reshape(-1)\n",
    "print(\"At position 3 should be a neg:\", Y[3])\n",
    "print(\"At position 2002 should be a pos:\", Y[2002])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the train dataset to build a Naive Bayes model. You can use the `sklearn` module `naive_bayes` from [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes) to accomplish this task. Carefully select the correct classifier for the data at hand by reading about different classification options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1343, 39373) (1343,)\n",
      "0.9657483246463142\n",
      "(662, 39373) (662,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\10705\\Desktop\\Notebooks\\labs_ml_naive_bayes\\submit.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/submit.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(X_test\u001b[39m.\u001b[39mtoarray()\u001b[39m.\u001b[39mshape, Y_test\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/submit.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39m#print(X_test.toarray())\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/submit.ipynb#ch0000007?line=8'>9</a>\u001b[0m \u001b[39m#clf.fit(X_test.toarray(), Y_test)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/submit.ipynb#ch0000007?line=9'>10</a>\u001b[0m clf\u001b[39m.\u001b[39;49mscore(X_test\u001b[39m.\u001b[39;49mtoarray(),Y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/submit.ipynb#ch0000007?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(test_score)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:646\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=620'>621</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=621'>622</a>\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=622'>623</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=641'>642</a>\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=642'>643</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=643'>644</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/base.py?line=645'>646</a>\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py:83\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=80'>81</a>\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=81'>82</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m---> <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=82'>83</a>\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m     <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=83'>84</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py:1461\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=1458'>1459</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=1459'>1460</a>\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=1460'>1461</a>\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=1461'>1462</a>\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   <a href='file:///c%3A/Users/10705/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/naive_bayes.py?line=1462'>1463</a>\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB()\n",
    "print(X_train.toarray().shape, Y_train.shape)\n",
    "clf.fit(X_train.toarray(), Y_train)\n",
    "train_score = clf.score(X_train.toarray(), Y_train)\n",
    "print(train_score)\n",
    "print(X_test.toarray().shape, Y_test.shape)\n",
    "#print(X_test.toarray())\n",
    "#clf.fit(X_test.toarray(), Y_test)\n",
    "test_score = clf.score(X_test.toarray(),Y_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, find 5 new movie reviews on the internet which include a numeric or star rating (known to be positive or negative), and try to classify them into positive/negative using your classifier. Report and discuss the results in a separate markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = path + \"/sample_new_reviews\"\n",
    "os.chdir(newpath)        \n",
    "newstringarray = []\n",
    "newfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{newpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        newstringarray.append(string)\n",
    "        newfilearray.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 39373)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = vectorizer.transform(newstringarray)\n",
    "vectorizer.get_feature_names_out()\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.07456958e-09 9.99999994e-01]\n",
      " [1.37553462e-10 1.00000000e+00]\n",
      " [5.22022285e-11 1.00000000e+00]\n",
      " [3.12585989e-12 1.00000000e+00]\n",
      " [1.63319736e-12 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(sample.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is horrible in this case, as it fails to predict the positive reviews. Also, the probability is really low for getting the actual result for the positive reviews and i am totally confused by that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a9095c727d968c862f8282558c4f789ee5b8decf02dbe2efbf73861a9bdff7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
