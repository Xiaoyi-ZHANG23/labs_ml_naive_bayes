{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, create a new Jupyther notebook, and implement a module that reads files and stores their content in 2 string arrays of file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os\n",
    "  \n",
    "# Folder Path\n",
    "path = \"C:/Users/10705/Desktop/Notebooks/labs_ml_naive_bayes/movies_reviews/movies_reviews\"\n",
    "negpath = path +\"/neg/\"\n",
    "pospath = path + \"/pos\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(negpath)\n",
    "  \n",
    "# Read text File\n",
    "  \n",
    "  \n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string\n",
    "    \n",
    "\n",
    "negstringarray = []\n",
    "negfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{negpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        negstringarray.append(string)\n",
    "        negfilearray.append(file)\n",
    "os.chdir(pospath)        \n",
    "posstringarray = []\n",
    "posfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{pospath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        posstringarray.append(string)\n",
    "        posfilearray.append(file)\n",
    "#print(array)\n",
    "newpath = path + \"/sample_new_reviews\"\n",
    "os.chdir(newpath)        \n",
    "newstringarray = []\n",
    "newfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{newpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        newstringarray.append(string)\n",
    "        newfilearray.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, you would need to convert the words in each document into a vector of word occurrences. \n",
    "You can use the code with stop words from the clustering demo or you can use the `sklearn` module `feature_extraction.text`, \n",
    "where you are interested in the `CountVectorizer` (for this one you would need to remove stop words) or in the `TfidfTransformer`. \n",
    "The latter assigns a score to each word based on its frequencies across all the documents, \n",
    "and thus the words that occur across all the documents (the stop words) get score zero, so there is no need to remove stop words. You can find a nice explanation and an example about tf/idf score [here](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3). Whatever vectorization technique you chose, you would need to explain it in your own words in a separate markdown cell in your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination: The method reads in the strings and build a disctionary after excluding the \"build-in\" stop words, which are the words that are meaningless. It then converts the string into the frequency array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot two teen couples go to a church party drink and then drive they get into an accident one of the guys dies but his girlfriend continues to see him in her life and has nightmares whats the deal watch the movie and sorta find out critique a mindfuck movie for the teen generation that touches on a very cool idea but presents it in a very bad package which is what makes this review an even harder one to write since i generally applaud films which attempt to break the mold mess with your head and such lost highway memento but there are good and bad ways of making all types of films and these folks just didnt snag this one correctly they seem to have taken this pretty neat concept but executed it terribly so what are the problems with the movie well its main problem is that its simply too jumbled it starts off normal but then downshifts into this fantasy world in which you as an audience member have no idea whats going on there are dreams there are characters coming back from the dead there are others who look like the dead there are strange apparitions there are disappearances there are a looooot of chase scenes there are tons of weird things that happen and most of it is simply not explained now i personally dont mind trying to unravel a film every now and then but when all it does is give me the same clue over and over again i get kind of fed up after a while which is this films biggest problem its obviously got this big secret to hide but it seems to want to hide it completely until its final five minutes and do they make things entertaining thrilling or even engaging in the meantime not really the sad part is that the arrow and i both dig on flicks like this so we actually figured most of it out by the halfway point so all of the strangeness after that did start to make a little bit of sense but it still didnt the make the film all that more entertaining i guess the bottom line with movies like this is that you should always make sure that the audience is into it even before they are given the secret password to enter your world of understanding i mean showing melissa sagemiller running away from visions for aboutminutes throughout the movie is just plain lazy okay we get it there are people chasing her and we dont know who they are do we really need to see it over and over again how about giving us different scenes offering further insight into all of the strangeness going down in the movie apparently the studio took this film away from its director and chopped it up themselves and it shows there mightve been a pretty decent teen mindfuck movie in here somewhere but i guess the suits decided that turning it into a music video with little edge would make more sense the actors are pretty good for the most part although wes bentley just seemed to be playing the exact same character that he did in american beauty only in a new neighborhood but my biggest kudos go out to sagemiller who holds her own throughout the entire film and actually has you feeling her characters unraveling overall the film doesnt stick because it doesnt entertain its confusing it rarely excites and it feels pretty redundant for most of its runtime despite a pretty cool ending and explanation to all of the craziness that came before it oh and by the way this is not a horror or teen slasher flick its just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids it also wrapped production two years ago and has been sitting on the shelves ever since whatever skip it wheres joblo coming from a nightmare of elm street 710 blair witch 710 the crow 910 the crow salvation 410 lost highway 1010 memento 1010 the others 910 stir of echoes 810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2005, 48103)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#replace everything not in english\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "#print(vectorizer.get_stop_words())\n",
    "allstringarray = negstringarray + posstringarray\n",
    "\n",
    "for i in range(len(allstringarray)):\n",
    "    review = allstringarray[i] \n",
    "    review = review.encode('ascii', 'ignore').decode()    \n",
    "    review = review.lower()\n",
    "    review = re.sub(r\"\\n\", \" \", review)\n",
    "    review = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', review) #remove word+num\n",
    "    review = re.sub(r'[^\\w\\s]', '', review) #remove unicode\n",
    "    review = re.sub(\"@\\S+\", \" \", review) #remove @\n",
    "    review = re.sub(\"https*\\S+\", \" \", review) #remove websites\n",
    "    review = re.sub(\"#\\S+\", \" \", review) #remove hashtags\n",
    "    review = re.sub(\"\\'\\w+\", '', review) #remove slashes\n",
    "    review = re.sub('\\s{2,}', \" \", review) #remove big spaces\n",
    "    # review = re.sub(r\"[0-9]\", \"\",review)#remove numbers\n",
    "    review = review.strip()\n",
    "    review = review.replace(\"_\", \"\")\n",
    "    allstringarray[i] = review\n",
    "print(allstringarray[0])\n",
    "X = vectorizer.fit_transform(allstringarray)\n",
    "\n",
    "X = X.toarray()\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a vector for each review, you can add the labels *pos* or *neg*, depending on the directory (as we did in cat/dog classification demo), and then divide the dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At position 3 should be a neg: 1.0\n",
      "At position 2004 should be a pos: 0.0\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y_orig = np.ones((1000,)) # 1 - 1000 are neg reviews so our label is 1\n",
    "Y_orig = np.concatenate((Y_orig, np.zeros((1005,)))) # 1000 - 2005 are pos reviews so our label is 0\n",
    "Y = Y_orig.reshape(-1)\n",
    "print(\"At position 3 should be a neg:\", Y[3])\n",
    "print(\"At position 2004 should be a pos:\", Y[2004])\n",
    "Y.shape\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the train dataset to build a Naive Bayes model. You can use the `sklearn` module `naive_bayes` from [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes) to accomplish this task. Carefully select the correct classifier for the data at hand by reading about different classification options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1343, 48103) (1343,)\n",
      "0.9865971705137752\n",
      "Total accuracy classification score with MultinomialNB: 0.8338368580060423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mclf = MultinomialNB()\n",
    "print(X_train.shape, Y_train.shape)\n",
    "mclf.fit(X_train, Y_train)\n",
    "train_score = mclf.score(X_train, Y_train)\n",
    "print(train_score)\n",
    "#test_score = mclf.score(X_test,Y_test)\n",
    "#print(test_score)\n",
    "\n",
    "#get test score\n",
    "from sklearn import metrics\n",
    "prediction_test2 = mclf.predict(X_test)\n",
    "score2 = metrics.accuracy_score(Y_test, prediction_test2)\n",
    "print('Total accuracy classification score with MultinomialNB: {}'.format(score2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, find 5 new movie reviews on the internet which include a numeric or star rating (known to be positive or negative), and try to classify them into positive/negative using your classifier. Report and discuss the results in a separate markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = path + \"/sample_new_reviews\"\n",
    "os.chdir(newpath)        \n",
    "newstringarray = []\n",
    "newfilearray = []\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{newpath}\\{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        string = read_text_file(file_path)\n",
    "        local = [file, string]\n",
    "        newstringarray.append(string)\n",
    "        newfilearray.append(file)\n",
    "for i in range(len(newstringarray)):\n",
    "    review = review.encode('ascii', 'ignore').decode()\n",
    "    review = newstringarray[i] \n",
    "    review = review.lower()\n",
    "    review = re.sub(r\"\\n\", \" \", review)\n",
    "    review = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', review)\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = re.sub(\"@\\S+\", \" \", review) #remoove @\n",
    "    review = re.sub(\"https*\\S+\", \" \", review)\n",
    "    review = re.sub(\"#\\S+\", \" \", review)\n",
    "    review = re.sub(\"\\'\\w+\", '', review)\n",
    "    review = re.sub('\\s{2,}', \" \", review)     \n",
    "    review = review.strip()\n",
    "    review = review.replace(\"_\", \"\")\n",
    "    newstringarray[i] = review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 48103)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = vectorizer.transform(newstringarray)\n",
    "vectorizer.get_feature_names_out()\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99735668 0.00264332]\n",
      " [0.91156945 0.08843055]\n",
      " [0.72074823 0.27925177]\n",
      " [0.81346709 0.18653291]\n",
      " [0.00366145 0.99633855]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(mclf.predict_proba(sample.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that for the first one it successfully recognizes a positive review, and the 5th one recognizes the negative review. The second, third and forth ones, although they recognizes the result correctly, the probability of wrong classification is still around or higher than 10% since those reviews uses negative words such as \"bored\", \"unfilled\" so it is distracting to the classification. However, the remaining part outweights the negative words and still result in a positive result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a9095c727d968c862f8282558c4f789ee5b8decf02dbe2efbf73861a9bdff7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
